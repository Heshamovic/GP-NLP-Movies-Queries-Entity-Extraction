{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AwUoC41CYUrBo4g3e9NLnComT88wfCoV",
      "authorship_tag": "ABX9TyM8dQFO1QMyG6UnYiFgSR+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heshamovic/GP-NLP-Movies-Queries-Entity-Extraction/blob/master/Entity_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ogRi6FDjO4",
        "colab_type": "text"
      },
      "source": [
        "# ***Entity Extraction***\n",
        "In this notebook we will illustrate how our model work to extract entity extraction for movies queries.\n",
        "\n",
        "We used [this blog](https://nlpforhackers.io/named-entity-extraction/) for helping in implementing our entity extraction.\n",
        "\n",
        "Our data didn't have all the indormation for the model to be trained. So, first we run the data on [Groningen Meaning Bank Corprus version 2.2.0](https://gmb.let.rug.nl/data.php) to fill the missing part in our data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnTuouEbF7FJ",
        "colab_type": "text"
      },
      "source": [
        "# ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUIOHlSqvtNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags, ChunkParserI\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.tag import ClassifierBasedTagger\n",
        "import os\n",
        "import collections\n",
        "import string\n",
        "import pickle\n",
        "import requests \n",
        "from collections import Iterable\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i42-PdqBF953",
        "colab_type": "text"
      },
      "source": [
        "# **Loading Data**\n",
        "We will use 2 datasets for this model gmb-2.2.0 to extract the NER for each sentence and [this data](https://groups.csail.mit.edu/sls/downloads/movie/) for movie queries intent.\n",
        "\n",
        "First we will need to download the gmb-2.2.0 zipped file.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Uw-OUVVTrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_url = \"https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\"\n",
        "    \n",
        "r = requests.get(file_url, stream = True)  \n",
        "\n",
        "with open(\"/content/drive/My Drive/gmb-2.2.0.zip\", \"wb\") as file:  \n",
        "    for block in r.iter_content(chunk_size = 1024): \n",
        "         if block:  \n",
        "             file.write(block)  \n",
        "\n",
        "!unzip '/content/drive/My Drive/gmb-2.2.0.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qZlyZ_cGaoJ",
        "colab_type": "text"
      },
      "source": [
        "# ***Preprocessing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtAyZo9wvuWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features(tokens, index, history):\n",
        "    \"\"\"\n",
        "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
        "    `index`   = the index of the token we want to extract features for\n",
        "    `history` = the previous predicted IOB tags\n",
        "    \"\"\"\n",
        " \n",
        "    # init the stemmer\n",
        "    stemmer = SnowballStemmer('english')\n",
        " \n",
        "    # Pad the sequence with placeholders\n",
        "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
        "    history = ['[START2]', '[START1]'] + list(history)\n",
        " \n",
        "    # shift the index with 2, to accommodate the padding\n",
        "    index += 2\n",
        " \n",
        "    word, pos = tokens[index]\n",
        "    prevword, prevpos = tokens[index - 1]\n",
        "    prevprevword, prevprevpos = tokens[index - 2]\n",
        "    nextword, nextpos = tokens[index + 1]\n",
        "    nextnextword, nextnextpos = tokens[index + 2]\n",
        "    previob = history[index - 1]\n",
        "    contains_dash = '-' in word\n",
        "    contains_dot = '.' in word\n",
        "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
        " \n",
        "    allcaps = word == word.capitalize()\n",
        "    capitalized = word[0] in string.ascii_uppercase\n",
        " \n",
        "    prevallcaps = prevword == prevword.capitalize()\n",
        "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
        " \n",
        "    nextallcaps = prevword == prevword.capitalize()\n",
        "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
        " \n",
        "    return {\n",
        "        'word': word,\n",
        "        'lemma': stemmer.stem(word),\n",
        "        'pos': pos,\n",
        "        'all-ascii': allascii,\n",
        " \n",
        "        'next-word': nextword,\n",
        "        'next-lemma': stemmer.stem(nextword),\n",
        "        'next-pos': nextpos,\n",
        " \n",
        "        'next-next-word': nextnextword,\n",
        "        'nextnextpos': nextnextpos,\n",
        " \n",
        "        'prev-word': prevword,\n",
        "        'prev-lemma': stemmer.stem(prevword),\n",
        "        'prev-pos': prevpos,\n",
        " \n",
        "        'prev-prev-word': prevprevword,\n",
        "        'prev-prev-pos': prevprevpos,\n",
        " \n",
        "        'prev-iob': previob,\n",
        " \n",
        "        'contains-dash': contains_dash,\n",
        "        'contains-dot': contains_dot,\n",
        " \n",
        "        'all-caps': allcaps,\n",
        "        'capitalized': capitalized,\n",
        " \n",
        "        'prev-all-caps': prevallcaps,\n",
        "        'prev-capitalized': prevcapitalized,\n",
        " \n",
        "        'next-all-caps': nextallcaps,\n",
        "        'next-capitalized': nextcapitalized,\n",
        "    }\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYPBWrDLC5cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_conll_iob(annotated_sentence):\n",
        "    \"\"\"\n",
        "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
        "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
        "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
        "    \"\"\"\n",
        "    proper_iob_tokens = []\n",
        "    for idx, annotated_token in enumerate(annotated_sentence):\n",
        "        tag, word, ner = annotated_token\n",
        " \n",
        "        if ner != 'O':\n",
        "            if idx == 0:\n",
        "                ner = \"B-\" + ner\n",
        "            elif annotated_sentence[idx - 1][2] == ner:\n",
        "                ner = \"I-\" + ner\n",
        "            else:\n",
        "                ner = \"B-\" + ner\n",
        "        proper_iob_tokens.append((tag, word, ner))\n",
        "    return proper_iob_tokens\n",
        "    \n",
        "def read_gmb(corpus_root):\n",
        "    for root, dirs, files in os.walk(corpus_root):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".tags\"):\n",
        "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
        "                    file_content = file_handle.read().decode('utf-8').strip()\n",
        "                    annotated_sentences = file_content.split('\\n\\n')\n",
        "                    for annotated_sentence in annotated_sentences:\n",
        "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
        " \n",
        "                        standard_form_tokens = []\n",
        " \n",
        "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
        "                            annotations = annotated_token.split('\\t')\n",
        "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
        " \n",
        "                            if ner != 'O':\n",
        "                                ner = ner.split('-')[0]\n",
        "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
        "                                tag = \"``\"\n",
        " \n",
        "                            standard_form_tokens.append((word, tag, ner))\n",
        " \n",
        "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
        "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
        "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
        "                        yield [((w, t), iob) for w, t, iob in conll_tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PnZAHixGw_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NamedEntityChunker(ChunkParserI):\n",
        "    def __init__(self, train_sents, **kwargs):\n",
        "        assert isinstance(train_sents, Iterable)\n",
        " \n",
        "        self.feature_detector = features\n",
        "        self.tagger = ClassifierBasedTagger(\n",
        "            train=train_sents,\n",
        "            feature_detector=features,\n",
        "            **kwargs)\n",
        " \n",
        "    def parse(self, tagged_sent):\n",
        "        chunks = self.tagger.tag(tagged_sent)\n",
        " \n",
        "        # Transform the result from [((w1, t1), iob1), ...] \n",
        "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
        "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
        " \n",
        "        # Transform the list of triplets to nltk.Tree format\n",
        "        return conlltags2tree(iob_triplets)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-EAmlPKYwuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareSentence(s):\n",
        "    ans = []\n",
        "    sent = ''\n",
        "    for i in s:\n",
        "        sent = sent + i[0] + ' ' \n",
        "    t = chunker.parse(pos_tag(word_tokenize(sent[:-1])))\n",
        "    for i, j in zip(s, t):\n",
        "        x = ''\n",
        "        if len(j) > 1:\n",
        "            x = j[1]\n",
        "        else:\n",
        "            start = False\n",
        "            for k in j[0]:\n",
        "                if start:\n",
        "                    x = x + k\n",
        "                if k == '/':\n",
        "                    start = True\n",
        "        l = [i[0], x, i[1]]\n",
        "        ans.append(l)\n",
        "    return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06syor3ykfuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_movies_data():\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Train.txt', 'r') as f:\n",
        "        x = f.readlines()\n",
        "    X_train = []\n",
        "    s = []\n",
        "    for i in x:\n",
        "        if len(i.split()) < 2:\n",
        "            X_train.append(prepareSentence(s))\n",
        "            s = []\n",
        "            continue\n",
        "        s.append((i.split()[1], i.split()[0]))\n",
        "    return X_train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z4IUPsvlsnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_movies_data(x_train):\n",
        "    for sentence in x_train:\n",
        "        standard_form_tokens = []\n",
        "        for w in sentence:\n",
        "            word, tag, ner = w[0], w[1], w[2]\n",
        "            if ner != 'O':\n",
        "                ner = ner.split('-')[1]\n",
        "            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
        "                tag = \"``\"\n",
        " \n",
        "            standard_form_tokens.append((word, tag, ner))\n",
        "        conll_tokens = to_conll_iob(standard_form_tokens)\n",
        "        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
        "        # Because the classfier expects a tuple as input, first item input, second the class\n",
        "        yield [((w, t), iob) for w, t, iob in conll_tokens]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i8VC1P-CNOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_root = \"gmb-2.2.0\"\n",
        "reader = read_gmb(corpus_root)\n",
        "data = list(reader)\n",
        "training_ratio = 0.7\n",
        "training_samples = data[:int(len(data) * training_ratio)]\n",
        "test_samples = data[int(len(data) * training_ratio):]\n",
        "chunker = NamedEntityChunker(training_samples)\n",
        "    \n",
        "x_train = prepare_movies_data()\n",
        "reader = read_movies_data(x_train)\n",
        "data = list(reader)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRJxPESgDOwI",
        "colab_type": "text"
      },
      "source": [
        "# ***Training***\n",
        "\n",
        "We combined all the data found on the link provided before to get large data of about 20000 sentence and tried several training to testing ratio and 84% was the one with best accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRmciWfipzDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ratio = 0.84\n",
        "training_samples = data[:int(len(data) * training_ratio)]\n",
        "test_samples = data[int(len(data) * training_ratio):]\n",
        "chunker = NamedEntityChunker(training_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdZ4iww6DUDR",
        "colab_type": "text"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgbjY5zLDdcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d183acdc-1505-406f-e3cc-fb28b045382f"
      },
      "source": [
        "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples])\n",
        "print(\"Training Ratio: {0}, Accuracy: {1}\".format(training_ratio, score.accuracy()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Ratio: 0.84, Accuracy: 0.7713279581885975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9q_jLmwI3gt",
        "colab_type": "text"
      },
      "source": [
        "Enter any query to see its entity extraction tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFmdrLloqp0g",
        "colab_type": "code",
        "outputId": "2723bca9-50bb-4749-f3aa-341b5faa4642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(chunker.parse(pos_tag(word_tokenize(\"show me romantic comedy movies\"))))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S show/VB me/PRP (GENRE romantic/JJ comedy/NN) movies/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWHluDUJ_RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}